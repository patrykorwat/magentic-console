{
  "models": [
    {
      "id": "SpeakLeash/bielik-11b-v3.0-instruct:Q4_K_M",
      "name": "Bielik 11B v3.0 (Q6_K)",
      "description": "Polski model językowy o wysokiej jakości. Najlepszy wybór dla zadań w języku polskim. Doskonały do analizy, rozumowania i pracy z MCP.",
      "size": "11B parameters (Q6_K quantization)",
      "capabilities": ["reasoning", "coding", "analysis", "mcp_tools", "polish_language"],
      "recommended": true
    },
    {
      "id": "qwen3:8b",
      "name": "Qwen 3 (8B)",
      "description": "Najlepszy balans jakości i szybkości. Doskonały do większości zadań.",
      "size": "8B parameters",
      "capabilities": ["reasoning", "coding", "analysis", "mcp_tools"],
      "recommended": false
    },
    {
      "id": "llama3.2:3b",
      "name": "Llama 3.2 (3B)",
      "description": "Bardzo szybki, lekki model. Dla prostych pytań i szybkich odpowiedzi.",
      "size": "3B parameters",
      "capabilities": ["simple_qa", "basic_analysis"]
    },
    {
      "id": "mistral:7b",
      "name": "Mistral (7B)",
      "description": "Silny w rozumowaniu logicznym i analizie. Dobry dla bardziej złożonych zadań.",
      "size": "7B parameters",
      "capabilities": ["reasoning", "analysis", "mcp_tools"]
    }
  ],
  "notes": [
    "Modele muszą być pobrane lokalnie: ollama pull <model_name>",
    "Dla Bielik: ollama pull SpeakLeash/bielik-11b-v3.0-instruct:Q4_K_M",
    "Większe modele (>8B) wymagają więcej RAM (Bielik 11B: ~8GB RAM)",
    "Wszystkie modele z 'mcp_tools' mogą używać narzędzi MCP"
  ]
}
